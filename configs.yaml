defaults:

  logdir: null
  traindir: null
  evaldir: null
  offline_traindir: ''
  offline_evaldir: ''
  seed: 0
  deterministic_run: False
  steps: 1e6
  parallel: False
  eval_every: 1e4
  eval_episode_num: 10
  log_every: 1e4
  reset_every: 0
  device: 'cuda:0'
  compile: True
  precision: 32
  debug: False
  video_pred_log: True

  # World Model Selection: 'dreamer' or 'vqvae'
  world_model: 'dreamer'

  # VQ-VAE World Model settings (used when world_model='vqvae')
  vqvae_embedding_dim: 32
  vqvae_num_embeddings: 128
  vqvae_hidden_channels: 256
  vqvae_commitment_cost: 0.25

  # Environment
  task: 'dmc_walker_walk'
  size: [64, 64]
  envs: 1
  action_repeat: 2
  time_limit: 1000
  grayscale: False
  prefill: 2500
  reward_EMA: True

  # Highway-env defaults (used when suite is 'highway')
  highway_obs_type: 'image'
  highway_action_type: 'discrete'
  highway_vehicles_count: 5

  # Model
  dyn_hidden: 512
  dyn_deter: 512
  dyn_stoch: 32
  dyn_discrete: 32
  dyn_rec_depth: 1
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  grad_heads: ['decoder', 'reward', 'cont']
  units: 512
  act: 'SiLU'
  norm: True
  encoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
  actor:
    {layers: 2, dist: 'normal', entropy: 3e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic:
    {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  reward_head:
    {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
  cont_head:
    {layers: 2, loss_scale: 1.0, outscale: 1.0}
  dyn_scale: 0.5
  rep_scale: 0.1
  kl_free: 1.0
  weight_decay: 0.0
  unimix_ratio: 0.01
  initial: 'learned'

  # Training
  batch_size: 16
  batch_length: 64
  train_ratio: 512
  pretrain: 100
  model_lr: 1e-4
  opt_eps: 1e-8
  grad_clip: 1000
  dataset_size: 1000000
  opt: 'adam'

  # Behavior.
  discount: 0.997
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: 0.0
  eval_state_mean: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 0
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  disag_target: 'stoch'
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_layers: 4
  disag_units: 400
  disag_action_cond: False

dmc_proprio:
  steps: 5e5
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: false
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}

dmc_vision:
  steps: 1e6
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: true
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

crafter:
  task: crafter_reward
  step: 1e6
  action_repeat: 1
  envs: 1
  train_ratio: 512
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'

atari100k:
  steps: 4e5
  envs: 1
  action_repeat: 4
  train_ratio: 1024
  video_pred_log: true
  eval_episode_num: 100
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  stickey: False
  lives: unused
  noops: 30
  resize: opencv
  actions: needed
  time_limit: 108000

minecraft:
  task: minecraft_diamond
  step: 1e8
  parallel: True
  envs: 16
  # no eval
  eval_episode_num: 0
  eval_every: 1e4
  action_repeat: 1
  train_ratio: 16
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath|obs_reward', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'
  break_speed: 100.0
  time_limit: 36000

memorymaze:
  steps: 1e8
  action_repeat: 2
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  task: 'memorymaze_9x9'

debug:
  debug: True
  pretrain: 1
  prefill: 1
  batch_size: 10
  batch_length: 20

# Highway-Env autonomous driving configurations
highway:
  task: highway_highway
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  time_limit: 200
  eval_every: 5e3
  log_every: 1e3
  video_pred_log: true
  size: [64, 64]
  grayscale: False
  encoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 32, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 32, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 2, dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  # Highway-env specific settings
  highway_obs_type: 'image'
  highway_action_type: 'discrete'
  # Vehicle density settings
  highway_vehicles_count: 50      # Number of vehicles (default was ~5, now 50)
  highway_vehicles_density: 1.5   # Density of vehicles on road
  # Reward shaping configuration
  highway_reward_shaping: True
  highway_reward_config:
    # 1) Speed – still important, but less overpowering
    high_speed_reward: 0.4            # from 0.6 -> 0.4
    reward_speed_range: [23.0, 27.0]  # ~83-97 km/h (matches traffic speed)
    # 2) Safety: MUCH stronger
    collision_reward: -5.0            # from -2.0 -> -5.0
    on_road_reward: 0.02
    # 3) Lane behavior – push harder to escape blocking
    lane_change_reward: 0.0
    smart_lane_change_reward: 0.25    # from 0.15 -> 0.25
    blocked_lane_penalty: 0.6         # from 0.30 -> 0.6
    overtake_reward: 0.2              # slightly up from 0.15
    # 4) Distance / tailgating – harsher
    min_safe_distance: 20.0           # from 15.0 -> 20.0
    safe_distance_penalty: 1.0        # from 0.40 -> 1.0 (2.5x)
    # 5) Lane "look ahead" / slow vehicle detection
    look_ahead_distance: 50.0
    slow_vehicle_threshold: 0.9       # from 0.85 -> 0.9
    # 6) Progress / heading shaping
    progress_reward_scale: 0.005
    heading_reward: 0.2
    # 7) Episode-level shaping
    success_reward: 1.0
    shaped_reward_weight: 0.85
    # Optional
    normalize_reward: False

highway_kinematics:
  task: highway_highway
  steps: 5e5
  action_repeat: 1
  envs: 1
  train_ratio: 512
  time_limit: 200
  video_pred_log: false
  encoder: {mlp_keys: 'vector', cnn_keys: '$^', mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: 'vector', cnn_keys: '$^', mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 2, dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  # Highway-env specific settings
  highway_obs_type: 'kinematics'
  highway_action_type: 'discrete'
  highway_vehicles_count: 5

highway_continuous:
  task: highway_highway
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 200
  video_pred_log: true
  size: [64, 64]
  grayscale: False
  encoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 32, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 32, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 2, dist: 'normal', entropy: 3e-4}
  imag_gradient: 'dynamics'
  # Highway-env specific settings
  highway_obs_type: 'image'
  highway_action_type: 'continuous'

intersection:
  task: highway_intersection
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 200
  video_pred_log: true
  size: [64, 64]
  grayscale: False
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  actor: {layers: 2, dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  highway_obs_type: 'image'
  highway_action_type: 'discrete'
  # Intersection uses initial_vehicle_count (vehicles_count is converted)
  highway_vehicles_count: 15        # initial_vehicle_count (max ~20)
  highway_vehicles_density: 1.2     # Converted to spawn_probability
  highway_reward_shaping: True
  highway_reward_config:
    high_speed_reward: 0.3
    reward_speed_range: [8, 15]
    collision_reward: -1.0
    safe_distance_reward: 0.15
    min_safe_distance: 10.0
    safe_distance_penalty: 0.4
    on_road_reward: 0.1
    heading_reward: 0.15
    survival_reward: 0.02
    success_reward: 1.0
    shaped_reward_weight: 0.8

parking:
  task: highway_parking
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 300
  video_pred_log: true
  size: [64, 64]
  grayscale: False
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  actor: {layers: 2, dist: 'normal', entropy: 3e-4}
  imag_gradient: 'dynamics'
  highway_obs_type: 'image'
  highway_action_type: 'continuous'
  highway_reward_shaping: True
  highway_reward_config:
    collision_reward: -1.0
    safe_distance_reward: 0.1
    min_safe_distance: 3.0
    safe_distance_penalty: 0.5
    heading_reward: 0.3
    on_road_reward: 0.2
    survival_reward: 0.01
    success_reward: 2.0
    shaped_reward_weight: 0.7

merge:
  task: highway_merge
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 200
  video_pred_log: true
  size: [64, 64]
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  actor: {layers: 2, dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  highway_obs_type: 'image'
  highway_action_type: 'discrete'
  highway_reward_shaping: True
  highway_reward_config:
    high_speed_reward: 0.35
    reward_speed_range: [20, 28]
    collision_reward: -1.0
    safe_distance_reward: 0.15
    min_safe_distance: 12.0
    safe_distance_penalty: 0.4
    lane_change_reward: -0.1
    on_road_reward: 0.1
    heading_reward: 0.1
    progress_reward_scale: 0.015
    survival_reward: 0.01
    success_reward: 0.8
    shaped_reward_weight: 0.8

roundabout:
  task: highway_roundabout
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 200
  video_pred_log: true
  size: [64, 64]
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  actor: {layers: 2, dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  highway_obs_type: 'image'
  highway_action_type: 'discrete'
  highway_reward_shaping: True
  highway_reward_config:
    high_speed_reward: 0.25
    reward_speed_range: [8, 15]
    collision_reward: -1.0
    safe_distance_reward: 0.2
    min_safe_distance: 8.0
    safe_distance_penalty: 0.4
    lane_change_reward: -0.03
    on_road_reward: 0.15
    heading_reward: 0.2
    progress_reward_scale: 0.01
    survival_reward: 0.02
    success_reward: 1.0
    shaped_reward_weight: 0.8

racetrack:
  task: highway_racetrack
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 500
  video_pred_log: true
  size: [64, 64]
  grayscale: False
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  actor: {layers: 2, dist: 'normal', entropy: 3e-4}
  imag_gradient: 'dynamics'
  highway_obs_type: 'image'
  highway_action_type: 'continuous'
  # Racetrack uses other_vehicles parameter (max ~10)
  highway_vehicles_count: 5         # other_vehicles on track
  highway_vehicles_density: 1.0
  highway_reward_shaping: True
  highway_reward_config:
    high_speed_reward: 0.5
    reward_speed_range: [15, 25]
    collision_reward: -1.0
    safe_distance_reward: 0.1
    min_safe_distance: 10.0
    safe_distance_penalty: 0.3
    on_road_reward: 0.2
    off_road_penalty: -0.8
    heading_reward: 0.25
    progress_reward_scale: 0.02
    survival_reward: 0.01
    success_reward: 1.0
    shaped_reward_weight: 0.85

# VQ-VAE World Model configurations
highway_vqvae:
  task: highway_highway
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 200
  video_pred_log: false
  size: [64, 64]
  grayscale: False
  # Use VQ-VAE world model instead of Dreamer
  world_model: 'vqvae'
  # VQ-VAE hyperparameters
  vqvae_embedding_dim: 32
  vqvae_num_embeddings: 128
  vqvae_hidden_channels: 256
  # Actor config for VQ-VAE (uses PPO internally)
  actor: {layers: 2, dist: 'onehot', std: 'none', lr: 2.5e-4, eps: 1e-5, grad_clip: 0.5}
  # Highway-env specific settings
  highway_obs_type: 'image'
  highway_action_type: 'discrete'
  highway_vehicles_count: 50
  highway_vehicles_density: 1.5
  highway_reward_shaping: True
  highway_reward_config:
    high_speed_reward: 0.4
    reward_speed_range: [23.0, 27.0]
    collision_reward: -5.0
    on_road_reward: 0.02
    lane_change_reward: 0.0
    smart_lane_change_reward: 0.25
    blocked_lane_penalty: 0.6
    overtake_reward: 0.2
    min_safe_distance: 20.0
    safe_distance_penalty: 1.0
    look_ahead_distance: 50.0
    slow_vehicle_threshold: 0.9
    progress_reward_scale: 0.005
    heading_reward: 0.2
    success_reward: 1.0
    shaped_reward_weight: 0.85
    normalize_reward: False

intersection_vqvae:
  task: highway_intersection
  steps: 1e5
  action_repeat: 1
  envs: 1
  train_ratio: 64
  eval_every: 5e3
  log_every: 1e3
  time_limit: 200
  video_pred_log: false
  size: [64, 64]
  grayscale: False
  world_model: 'vqvae'
  vqvae_embedding_dim: 32
  vqvae_num_embeddings: 128
  vqvae_hidden_channels: 256
  actor: {layers: 2, dist: 'onehot', std: 'none', lr: 2.5e-4, eps: 1e-5, grad_clip: 0.5}
  highway_obs_type: 'image'
  highway_action_type: 'discrete'
  highway_vehicles_count: 15
  highway_vehicles_density: 1.2
  highway_reward_shaping: True
  highway_reward_config:
    high_speed_reward: 0.3
    reward_speed_range: [8, 15]
    collision_reward: -1.0
    safe_distance_reward: 0.15
    min_safe_distance: 10.0
    safe_distance_penalty: 0.4
    on_road_reward: 0.1
    heading_reward: 0.15
    survival_reward: 0.02
    success_reward: 1.0
    shaped_reward_weight: 0.8
